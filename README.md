# Notes

### Open-Data LLMs

| Model   | Link                                                    | Pre-training  | Link                                                        | Weights Used                  |
|---------|---------------------------------------------------------|---------------|-------------------------------------------------------------|-------------------------------|
| SmolLM  | https://huggingface.co/collections/HuggingFaceTB/smollm | SmolLM Corpus | https://huggingface.co/datasets/HuggingFaceTB/smollm-corpus | 165M, 36M, 1.7B, all instruct |
| Olmo 3  | https://allenai.org/olmo                                | Dolma 3       | https://huggingface.co/datasets/allenai/dolma3_mix-6T-1025  |                               |
| K2-V2   | https://huggingface.co/LLM360/K2                        | TxT360        | https://github.com/LLM360/TxT360                            |                               |
| Pleias  | https://huggingface.co/collections/PleIAs/common-models | Common Corpus | https://huggingface.co/datasets/PleIAs/common_corpus        |                               |
| Apertus | https://huggingface.co/swiss-ai/models                  | Mix           | -                                                           |                               |
| Marin   | https://huggingface.co/marin-community/marin-32b-base   | Mix           | -                                                           |                               |

Apertus and Marin use a mixture of datasets rather than one dataset.


